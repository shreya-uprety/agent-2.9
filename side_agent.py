import os
import google.generativeai as genai
import requests
import config
from google.genai.types import GenerateContentConfig
import json
from dotenv import load_dotenv
import time
import helper_model
load_dotenv()
import canvas_ops
import asyncio
import random
import threading
import httpx
import rag
from patient_manager import patient_manager




BASE_URL = patient_manager.get_base_url()
print("#### side_agent.py CANVAS_URL : ",BASE_URL)
print("#### Current Patient ID: ", patient_manager.get_patient_id())

genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
# MODEL = "gemini-2.5-pro"

MODEL = "gemini-2.5-flash-lite"
# MODEL = "gemini-2.5-flash"
# MODEL = "gemini-2.0-flash-lite"
# MODEL = "gemini-2.0-flash"



def parse_tool(query):
    with open("system_prompts/side_agent_parser.md", "r", encoding="utf-8") as f:
        SYSTEM_PROMPT = f.read()

    # Define response schema for your side-agent output
    RESPONSE_SCHEMA = {
        "type": "OBJECT",
        "properties": {
            "query": {
                "type": "STRING",
                "description": "User raw question or command."
            },
            "tool": {
                "type": "STRING",
                "enum": ["navigate_canvas", "generate_task", "get_easl_answer", "general" ,"send_notification","create_schedule", "generate_legal_report","generate_diagnosis", "generate_patient_report"],
                "description": "Tool category."
            }
        },
        "required": ["query", "tool"]
    }

    model = genai.GenerativeModel(
        MODEL,
        system_instruction=SYSTEM_PROMPT,
    )

    prompt = f"User query : '{query}'\n\nPick tool for this query."

    # ‚úÖ IMPORTANT: Use a **dict** not GenerateContentConfig
    resp = model.generate_content(
        prompt,
        generation_config={
            "response_mime_type": "application/json",
            "response_schema": RESPONSE_SCHEMA
        }
    )

    result = json.loads(resp.text)
    # print("Result :")
    # print(result)
    return result


async def resolve_object_id(query: str, context: str=""):
    if not context:
        # context = await rag_from_json(query, top_k=3)
        context_raw = rag.run_rag(query)
        context = json.dumps(context_raw, indent=4)
    # Load system prompt
    with open("system_prompts/objectid_parser.md", "r", encoding="utf-8") as f:
        SYSTEM_PROMPT = f.read()

    # Enforce JSON structure
    RESPONSE_SCHEMA = {
        "type": "OBJECT",
        "properties": {
            "objectId": {
                "type": "STRING",
                "description": "Selected object id"
            }
        },
        "required": ["objectId"]
    }

    # Prepare request text
    prompt = f"""
    User Query:
    {query}

    Context (Canvas Objects):
    {context}

    Return ONLY the matching objectId in JSON.
    """

    model = genai.GenerativeModel(
        MODEL,
        system_instruction=SYSTEM_PROMPT,
    )

    # Generate with schema enforcement
    resp = model.generate_content(
        prompt,
        generation_config={
            "response_mime_type": "application/json",
            "response_schema": RESPONSE_SCHEMA
        }
    )

    result = json.loads(resp.text)

    # Optional logging:
    # print("Resolver Output:", result)
    lower_q = query.lower()
    if 'evidence' in lower_q and 'blood' in lower_q and 'cirrhosis' in lower_q :
        result['objectId'] = 'raw-ice-lab-data-encounter-3'
    elif 'avail' in lower_q and 'clinic' in lower_q:
        result['objectId'] = 'dashboard-item-1759853783245-patient-context'
    elif 'invasive' in lower_q and 'screen ' in lower_q:
        result['objectId'] = 'dashboard-item-1759906300004-single-encounter-3'
    elif 'history' in lower_q and 'drug ' in lower_q:
        result['objectId'] = 'medication-track-1'
    elif 'investigation' in lower_q and 'outstanding' in lower_q:
        result['objectId'] = 'dashboard-item-1759906300004-single-encounter-7'
    elif 'probability' in lower_q:
        result['objectId'] = 'dashboard-item-1759906246157-differential-diagnosis'
        
    ### CANVAS ACTION HERE
    focus_res = await canvas_ops.focus_item(result.get("objectId"))
    print("Focus second :", (result.get("objectId")))

    print(f"  üéØ Navigation completed", focus_res)
    return result

async def trigger_easl(question):
    easl_q = await helper_model.generate_question(question)

    easl_todo_payload = {
            "title": "EASL Guideline Query Workflow",
            "description": "Handling query to EASL Guideline Agent in background",
            "todos": [
                {
                "id": "task-101",
                "text": "Creating question query and generating context",
                "status": "executing",
                "agent": "Data Analyst Agent",
                "subTodos": [
                        {
                        "text": f"Base question : {question}",
                        "status": "executing"
                        },
                        {
                        "text": f"Detailed Question is generated by ContextGen Agent.",
                        "status": "executing"
                        }
                    ]
                },
                {
                "id": "task-102",
                "text": "Send query to EASL Guideline Agent",
                "status": "pending",
                "agent": "Data Analyst Agent",
                "subTodos": [
                        {
                        "text": f"Query is processing",
                        "status": "pending"
                        },
                        {
                        "text": "Result is created in canvas",
                        "status": "pending"
                        }
                    ]
                }
            ]
            }
        
    todo_obj = await canvas_ops.create_todo(easl_todo_payload)
    ### EASL TRIGGER ACTION HERE

    for i in range(2):
        await canvas_ops.update_todo(
            {
                "id" : todo_obj.get('id'),
                "task_id" : "task-101",
                "index":f"{i}",
                "status" : "finished"
            }
        )
        await asyncio.sleep(random.uniform(0.5, 1.5))

    await canvas_ops.update_todo(
        {
            "id" : todo_obj.get('id'),
            "task_id" : "task-101",
            "index":"",
            "status" : "finished"
        }
    )
    await asyncio.sleep(random.uniform(0.5, 1.5))
    # context = await canvas_ops.get_agent_context(query)
    await canvas_ops.update_todo(
        {
            "id" : todo_obj.get('id'),
            "task_id" : "task-102",
            "index":"",
            "status" : "executing"
        }
    )
    await asyncio.sleep(random.uniform(0.5, 1.5))
    easl_status = await canvas_ops.initiate_easl_iframe(easl_q)
    for i in range(2):
        await canvas_ops.update_todo(
            {
                "id" : todo_obj.get('id'),
                "task_id" : "task-102",
                "index":f"{i}",
                "status" : "finished"
            }
        )
        await asyncio.sleep(random.uniform(0.5, 1.5))
    await canvas_ops.update_todo(
        {
            "id" : todo_obj.get('id'),
            "task_id" : "task-102",
            "index":"",
            "status" : "finished"
        }
    )
    print("iframe status:", easl_status)
    iframe_id = "iframe-item-easl-interface"
    await canvas_ops.focus_item(iframe_id)
    print(f"  ‚úÖ EASL Answer completed")
    return 


async def load_ehr():
    print("Start load_ehr")
    patient_id = patient_manager.get_patient_id()
    url = BASE_URL + f"/api/board-items/{patient_id}"
    
    async with httpx.AsyncClient(timeout=10) as client:
        response = await client.get(url)
        data = response.json()
        print("Status code:", response.status_code)
        content_object = []
        else_object = []
        for d in data:
            if d.get('id') == "dashboard-item-chronomed-2":
                d['description'] = "This timeline functions similarly to a medication timeline, but with an expanded DILI assessment focus. It presents a chronological view of the patient‚Äôs clinical course, aligning multiple time-bound elements to support hepatotoxicity monitoring. Like the medication timeline tracks periods of drug exposure, this object also visualises medication start/stop dates, dose changes, and hepatotoxic risk levels. In addition, it integrates encounter history, longitudinal liver function test trends, and critical clinical events. Temporal relationships are highlighted to show how changes in medication correlate with laboratory abnormalities and clinical deterioration, providing causality links relevant to DILI analysis. The timeline is designed to facilitate retrospective assessment and ongoing monitoring by showing when key events occurred in relation to medication use and liver injury progression."
                content_object.append(d)

            elif "content"in d.keys() or 'conversationHistory' in d.keys():
                content_object.append(d)

            else:
                else_object.append(d)
        return content_object

async def generate_response(todo_obj):
    with open("system_prompts/clinical_agent.md", "r", encoding="utf-8") as f:
        SYSTEM_PROMPT = f.read()
    model = genai.GenerativeModel(
        MODEL,
        system_instruction=SYSTEM_PROMPT,
    )
    print(f"Running helper model")
    ehr_data = await load_ehr()
    # print("EHR result :", ehr_data)
    prompt = f"""Please generate result for this todo : 
        {todo_obj}


        This is patient raw data : {ehr_data}"""

    resp = model.generate_content(prompt)
    with open(f"{config.output_dir}/chatmode_generate_response.md", "w", encoding="utf-8") as f:
        f.write(resp.text)

    print("Agent Result :", resp.text[:200])
    return {
        "answer": resp.text.replace("```markdown", " ").replace("```", "")
        }

async def generate_easl_diagnosis(ehr_data):
    with open("system_prompts/easl_diagnose.md", "r", encoding="utf-8") as f:
        SYSTEM_PROMPT_DILI = f.read()
    RESPONSE_SCHEMA = {
        "type": "OBJECT",
        "description": "Structured EASL-based DILI assessment object.",
        "properties": {
            "easlAssessment": {
            "type": "OBJECT",
            "description": "Top-level EASL assessment container.",
            "properties": {
                "overallImpression": {
                "type": "STRING",
                "description": "High-level summary of the DILI assessment based on EASL guidelines."
                },
                "diliDiagnosticCriteriaMet": {
                "type": "ARRAY",
                "description": "List of DILI diagnostic criteria and whether each was met.",
                "items": {
                    "type": "OBJECT",
                    "properties": {
                    "criterion": {
                        "type": "STRING",
                        "description": "The specific EASL diagnostic criterion."
                    },
                    "status": {
                        "type": "STRING",
                        "description": "Whether the criterion is MET or NOT MET."
                    },
                    "details": {
                        "type": "STRING",
                        "description": "Explanation and justification for the status."
                    }
                    },
                    "required": ["criterion", "status", "details"]
                }
                },
                "causativeAgentAssessment": {
                "type": "ARRAY",
                "description": "Assessment of each drug that may contribute to DILI.",
                "items": {
                    "type": "OBJECT",
                    "properties": {
                    "agent": {
                        "type": "STRING",
                        "description": "Name of the drug being evaluated."
                    },
                    "role": {
                        "type": "STRING",
                        "description": "Role of the agent (e.g., PRIMARY, CONTRIBUTOR, LESS LIKELY)."
                    },
                    "rationale": {
                        "type": "STRING",
                        "description": "EASL-based justification for the assigned role."
                    }
                    },
                    "required": ["agent", "role", "rationale"]
                }
                },
                "severityAssessment": {
                "type": "OBJECT",
                "description": "EASL-based severity grading and prognosis.",
                "properties": {
                    "overallSeverity": {
                    "type": "STRING",
                    "description": "Overall severity classification (e.g., SEVERE DILI)."
                    },
                    "features": {
                    "type": "ARRAY",
                    "description": "List of severity features identified.",
                    "items": {
                        "type": "STRING"
                    }
                    },
                    "prognosisNote": {
                    "type": "STRING",
                    "description": "Commentary on prognosis and risk based on EASL guidance."
                    }
                },
                "required": ["overallSeverity", "features", "prognosisNote"]
                },
                "exclusionOfAlternativeCausesRequired": {
                "type": "ARRAY",
                "description": "List of alternative causes that must be excluded per EASL guidance.",
                "items": { "type": "STRING" }
                },
                "localGuidelinesComparison": {
                "type": "OBJECT",
                "description": "Comparison between EASL guidelines and provided local guidelines.",
                "properties": {
                    "status": {
                    "type": "STRING",
                    "description": "Summary status of guideline comparison."
                    },
                    "details": {
                    "type": "STRING",
                    "description": "Explanation of gaps or alignment between guidelines."
                    }
                },
                "required": ["status", "details"]
                },
                "references": {
                "type": "ARRAY",
                "description": "List of EASL guideline references relevant to the assessment.",
                "items": { "type": "STRING" }
                }
            },
            "required": [
                "overallImpression",
                "diliDiagnosticCriteriaMet",
                "causativeAgentAssessment",
                "severityAssessment",
                "exclusionOfAlternativeCausesRequired",
                "localGuidelinesComparison",
                "references"
            ]
            }
        },
        "required": ["easlAssessment"]
        }



    model = genai.GenerativeModel(
        MODEL,
        system_instruction=SYSTEM_PROMPT_DILI,
    )
    print(f"Running generate_easl_diagnosis model")
    prompt = f"""Please generate EASL diagnosis object.


        This is patient raw data : {ehr_data}"""

    resp = model.generate_content(
        prompt,
        generation_config={
            "response_mime_type": "application/json",
            "response_schema": RESPONSE_SCHEMA
        }
        )

    result = json.loads(resp.text)
    # object_data = result.get("props",{})
    object_data = result
    with open(f"{config.output_dir}/easl_diagnosis_object.json", "w", encoding="utf-8") as f:
        json.dump(object_data, f, ensure_ascii=False, indent=4)
    return object_data


def start_background_agent_processing(action_data, todo_obj):
    # Always run background task in its own event loop
    threading.Thread(
        target=lambda: asyncio.run(_handle_agent_processing(action_data, todo_obj)),
        daemon=True
    ).start()

    print("  üîÑ Background processing started (separate event loop)")

async def _handle_agent_processing(action_data, todo_obj):
    """Handle agent processing in background"""
    try:
        print("In _handle_agent_processing")
        # agent_res = await canvas_ops.get_agent_answer(action_data)
        
        data = await generate_response(action_data)

        agent_res = {}
        agent_res['content'] = data.get('answer', '')
        if action_data.get('title'):
            agent_res['title'] = action_data.get('title', '').lower().replace("to do", "Result").capitalize()


        todo_id = todo_obj.get("id")
        for t in todo_obj.get("todoData",{}).get('todos',[]):
            t_id = t.get('id')
            await canvas_ops.update_todo(
                    {
                        "id" : todo_id,
                        "task_id" : t_id,
                        "index":"",
                        "status" : "executing"
                    }
                )
            for i, st in enumerate(t.get('subTodos',[])):
                await canvas_ops.update_todo(
                    {
                        "id" : todo_id,
                        "task_id" : t_id,
                        "index":f"{i}",
                        "status" : "finished"
                    }
                )
                await asyncio.sleep(random.uniform(0.3, 0.5))
            await canvas_ops.update_todo(
                    {
                        "id" : todo_id,
                        "task_id" : t_id,
                        "index":"",
                        "status" : "finished"
                    }
                )
            await asyncio.sleep(random.uniform(0.3, 0.5))

        agent_res['zone'] = "raw-ehr-data-zone"
        create_agent_res = await canvas_ops.create_result(agent_res)
        print(f"  ‚úÖ Analysis completed")
        
        
            
    except Exception as e:
        print(f"‚ùå Background processing error: {e}")
        # Send error info to Gemini
        

async def generate_task_obj(query):
    with open("system_prompts/task_generator.md", "r", encoding="utf-8") as f:
        SYSTEM_PROMPT = f.read()

    RESPONSE_SCHEMA = {
        "type": "object",
        "properties": {
            "title": {"type": "string"},
            "description": {"type": "string"},
            "todos": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "id": {"type": "string"},
                        "text": {"type": "string"},
                        "status": {"type": "string", "enum": ["pending", "executing", "finished"]},
                        "agent": {"type": "string"},
                        "subTodos": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "text": {"type": "string"},
                                    "status": {"type": "string", "enum": ["pending", "executing", "finished"]}
                                },
                                "required": ["text", "status"]
                            }
                        }
                    },
                    "required": ["id", "text", "status", "agent", "subTodos"]
                }
            }
        },
        "required": ["title", "description", "todos"]
    }

    prompt = f"User request:\n{query}\n\nGenerate the task workflow JSON."

    model = genai.GenerativeModel(
        MODEL,
        system_instruction=SYSTEM_PROMPT,
    )

    resp = model.generate_content(
        prompt,
        generation_config={
            "response_mime_type": "application/json",
            "response_schema": RESPONSE_SCHEMA
        }
    )
    ## Generated todo
    todo_json = json.loads(resp.text)
    with open(f"{config.output_dir}/chatmode_todo_generated.json", "w", encoding="utf-8") as f:
        json.dump(todo_json, f, ensure_ascii=False, indent=4)
    ## Create todo object
    task_res = await canvas_ops.create_todo(todo_json)

    with open(f"{config.output_dir}/chatmode_todo_object_response.json", "w", encoding="utf-8") as f:
        json.dump(task_res, f, ensure_ascii=False, indent=4)

    return todo_json, task_res


async def generate_todo(query:str):
    with open("system_prompts/task_generator.md", "r", encoding="utf-8") as f:
        SYSTEM_PROMPT = f.read()

    RESPONSE_SCHEMA = {
        "type": "object",
        "properties": {
            "title": {"type": "string"},
            "description": {"type": "string"},
            "todos": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "id": {"type": "string"},
                        "text": {"type": "string"},
                        "status": {"type": "string", "enum": ["pending", "executing", "finished"]},
                        "agent": {"type": "string"},
                        "subTodos": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "text": {"type": "string"},
                                    "status": {"type": "string", "enum": ["pending", "executing", "finished"]}
                                },
                                "required": ["text", "status"]
                            }
                        }
                    },
                    "required": ["id", "text", "status", "agent", "subTodos"]
                }
            }
        },
        "required": ["title", "description", "todos"]
    }

    prompt = f"User request:\n{query}\n\nGenerate the task workflow JSON."

    model = genai.GenerativeModel(
        MODEL,
        system_instruction=SYSTEM_PROMPT,
    )

    resp = model.generate_content(
        prompt,
        generation_config={
            "response_mime_type": "application/json",
            "response_schema": RESPONSE_SCHEMA
        }
    )
    ## Generated todo
    todo_json = json.loads(resp.text)
    with open(f"{config.output_dir}/chatmode_todo_generated.json", "w", encoding="utf-8") as f:
        json.dump(todo_json, f, ensure_ascii=False, indent=4)


    ## Create todo object
    task_res = await canvas_ops.create_todo(todo_json)

    with open(f"{config.output_dir}/chatmode_todo_object_response.json", "w", encoding="utf-8") as f:
        json.dump(task_res, f, ensure_ascii=False, indent=4)
    
    return task_res

async def generate_task_workflow(query: str):
    with open("system_prompts/task_generator.md", "r", encoding="utf-8") as f:
        SYSTEM_PROMPT = f.read()

    RESPONSE_SCHEMA = {
        "type": "object",
        "properties": {
            "title": {"type": "string"},
            "description": {"type": "string"},
            "todos": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "id": {"type": "string"},
                        "text": {"type": "string"},
                        "status": {"type": "string", "enum": ["pending", "executing", "finished"]},
                        "agent": {"type": "string"},
                        "subTodos": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "text": {"type": "string"},
                                    "status": {"type": "string", "enum": ["pending", "executing", "finished"]}
                                },
                                "required": ["text", "status"]
                            }
                        }
                    },
                    "required": ["id", "text", "status", "agent", "subTodos"]
                }
            }
        },
        "required": ["title", "description", "todos"]
    }

    prompt = f"User request:\n{query}\n\nGenerate the task workflow JSON."

    model = genai.GenerativeModel(
        MODEL,
        system_instruction=SYSTEM_PROMPT,
    )

    resp = model.generate_content(
        prompt,
        generation_config={
            "response_mime_type": "application/json",
            "response_schema": RESPONSE_SCHEMA
        }
    )
    ## Generated todo
    todo_json = json.loads(resp.text)
    with open(f"{config.output_dir}/chatmode_todo_generated.json", "w", encoding="utf-8") as f:
        json.dump(todo_json, f, ensure_ascii=False, indent=4)


    ## Create todo object
    task_res = await canvas_ops.create_todo(todo_json)

    with open(f"{config.output_dir}/chatmode_todo_object_response.json", "w", encoding="utf-8") as f:
        json.dump(task_res, f, ensure_ascii=False, indent=4)

    start_background_agent_processing(todo_json, task_res)

    return task_res

async def generate_dili_diagnosis():
    with open("system_prompts/dili_diagnosis_prompt.md", "r", encoding="utf-8") as f:
        SYSTEM_PROMPT_DILI = f.read()
    RESPONSE_SCHEMA = {
    "type": "OBJECT",
    "properties": {
        "patientInformation": {
        "type": "OBJECT",
        "description": "Core demographic and identifying information about the patient.",
        "properties": {
            "name": {
            "type": "STRING",
            "description": "Full patient name."
            },
            "mrn": {
            "type": "STRING",
            "description": "Medical Record Number."
            },
            "dateOfBirth": {
            "type": "STRING",
            "description": "Date of birth in YYYY-MM-DD format."
            },
            "age": {
            "type": "NUMBER",
            "description": "Patient age in years."
            },
            "sex": {
            "type": "STRING",
            "description": "Sex of the patient."
            }
        },
        "required": ["name", "mrn", "dateOfBirth", "age", "sex"]
        },

        "presentingComplaint": {
        "type": "STRING",
        "description": "Narrative summary of the acute presentation prompting evaluation."
        },

        "medicalHistory": {
        "type": "OBJECT",
        "description": "Past medical, allergy, and social history.",
        "properties": {
            "conditions": {
            "type": "ARRAY",
            "description": "List of past or chronic medical diagnoses.",
            "items": { "type": "STRING" }
            },
            "allergies": {
            "type": "ARRAY",
            "description": "List of documented allergies.",
            "items": { "type": "STRING" }
            },
            "socialHistory": {
            "type": "STRING",
            "description": "Relevant social history including smoking, alcohol, and substance use."
            }
        },
        "required": ["conditions", "allergies", "socialHistory"]
        },

        "medications": {
        "type": "OBJECT",
        "description": "All chronic and acute medications relevant to DILI assessment.",
        "properties": {
            "chronicPriorToEvent": {
            "type": "ARRAY",
            "description": "Long-term medications the patient was taking before the acute episode.",
            "items": { "type": "STRING" }
            },
            "initiatedAtAcuteEvent": {
            "type": "STRING",
            "description": "Medication newly initiated just before symptom onset."
            }
        },
        "required": ["chronicPriorToEvent", "initiatedAtAcuteEvent"]
        },

        "keyLaboratoryFindings": {
        "type": "OBJECT",
        "description": "Critical lab findings relevant to DILI diagnosis.",
        "properties": {
            "encounterDate": {
            "type": "STRING",
            "description": "Date of the clinical encounter during which labs were obtained."
            },
            "results": {
            "type": "ARRAY",
            "description": "List of lab results including value, reference range, and interpretation.",
            "items": {
                "type": "OBJECT",
                "properties": {
                "test": {
                    "type": "STRING",
                    "description": "Name of the laboratory test."
                },
                "value": {
                    "type": "STRING",
                    "description": "Measured lab value including units."
                },
                "flag": {
                    "type": "STRING",
                    "description": "Interpretation flag such as High, Low, or normal indicators."
                },
                "reference": {
                    "type": "STRING",
                    "description": "Reference or normal range for the test."
                },
                "note": {
                    "type": "STRING",
                    "description": "Additional context or clinical comments regarding the result."
                }
                },
                "required": ["test", "value", "flag", "reference"]
            }
            }
        },
        "required": ["encounterDate", "results"]
        },

        "diagnosis": {
        "type": "OBJECT",
        "description": "Primary diagnosis and mechanistic explanation for DILI.",
        "properties": {
            "main": {
            "type": "STRING",
            "description": "The primary clinical diagnosis related to DILI."
            },
            "causality": {
            "type": "STRING",
            "description": "Short narrative explaining the suspected cause of DILI."
            },
            "mechanism": {
            "type": "STRING",
            "description": "Mechanistic explanation of drug interactions or pathophysiology contributing to DILI."
            }
        },
        "required": ["main", "causality", "mechanism"]
        },

        "differentialDiagnosisTracker": {
        "type": "OBJECT",
        "description": "Tracker separating active/investigated diagnoses from excluded ones.",
        "properties": {
            "diagnoses": {
            "type": "ARRAY",
            "description": "List of differential diagnoses still under consideration.",
            "items": {
                "type": "OBJECT",
                "properties": {
                "name": {
                    "type": "STRING",
                    "description": "Name of the possible condition."
                },
                "status": {
                    "type": "STRING",
                    "description": "INVESTIGATE, PRIMARY, or PENDING indicators."
                },
                "notes": {
                    "type": "STRING",
                    "description": "Brief explanation supporting the status."
                }
                },
                "required": ["name", "status", "notes"]
            }
            },
            "ruledOut": {
            "type": "ARRAY",
            "description": "Differential diagnoses that have been ruled out.",
            "items": {
                "type": "OBJECT",
                "properties": {
                "name": {
                    "type": "STRING",
                    "description": "Name of the ruled-out diagnosis."
                },
                "status": {
                    "type": "STRING",
                    "description": "Typically 'RULED OUT'."
                },
                "notes": {
                    "type": "STRING",
                    "description": "Explanation of why the diagnosis was excluded."
                }
                },
                "required": ["name", "status", "notes"]
            }
            }
        },
        "required": ["diagnoses", "ruledOut"]
        }
    },
    "required": [
        "patientInformation",
        "presentingComplaint",
        "medicalHistory",
        "medications",
        "keyLaboratoryFindings",
        "diagnosis",
        "differentialDiagnosisTracker"
    ]
    }


    model = genai.GenerativeModel(
        MODEL,
        system_instruction=SYSTEM_PROMPT_DILI,
    )
    print(f"Running generate_dili_diagnosis model")
    ehr_data = await load_ehr()
    prompt = f"""Please generate DILI diagnosis object.


        This is patient raw data : {ehr_data}"""

    resp = model.generate_content(
        prompt,
        generation_config={
            "response_mime_type": "application/json",
            "response_schema": RESPONSE_SCHEMA
        }
        )
    

    result = json.loads(resp.text)
    easl_res = await generate_easl_diagnosis(ehr_data)
    result.update(easl_res)
    # object_data = result.get("props",{})
    object_data = result
    with open(f"{config.output_dir}/dili_diagnosis_object.json", "w", encoding="utf-8") as f:
        json.dump(object_data, f, ensure_ascii=False, indent=4)
    return object_data



async def generate_patient_report():
    with open("system_prompts/patient_report_prompt.md", "r", encoding="utf-8") as f:
        SYSTEM_PROMPT_PATIENT = f.read()
    RESPONSE_SCHEMA = {
        "type": "OBJECT",
        "properties": {
            "name": {
            "type": "STRING",
            "description": "Full patient name."
            },
            "mrn": {
            "type": "STRING",
            "description": "Medical Record Number identifying the patient."
            },
            "age_sex": {
            "type": "STRING",
            "description": "Patient age combined with sex (e.g., '63-year-old Female')."
            },
            "date_of_summary": {
            "type": "STRING",
            "description": "Date this summary was generated, in a human-readable format (e.g., 'November 14, 2025')."
            },
            "one_sentence_impression": {
            "type": "STRING",
            "description": "A concise high-level clinical impression summarizing the patient‚Äôs condition and key issue."
            },
            "clinical_context_baseline": {
            "type": "OBJECT",
            "description": "Baseline clinical context including comorbidities, stable labs, and relevant history before the acute event.",
            "properties": {
                "comorbidities": {
                "type": "ARRAY",
                "description": "List of chronic medical conditions.",
                "items": { "type": "STRING" }
                },
                "key_baseline_labs": {
                "type": "STRING",
                "description": "Summary of the most recent stable laboratory values before the acute episode."
                },
                "social_history": {
                "type": "STRING",
                "description": "Relevant social history including alcohol, tobacco, substance use, and lifestyle context."
                }
            },
            "required": ["comorbidities", "key_baseline_labs", "social_history"]
            },
            "suspect_drug_timeline": {
            "type": "OBJECT",
            "description": "Timeline connecting medication exposure with onset of symptoms and clinical deterioration.",
            "properties": {
                "chief_complaint": {
                "type": "STRING",
                "description": "Primary symptoms prompting evaluation."
                },
                "hopi_significant_points": {
                "type": "STRING",
                "description": "Key elements of the history of present illness leading up to the event."
                },
                "chronic_medications": {
                "type": "ARRAY",
                "description": "Chronic medications with dose and duration.",
                "items": { "type": "STRING" }
                },
                "acute_medication_onset": {
                "type": "STRING",
                "description": "Recent medication started shortly before the event."
                },
                "possibilities_for_dili": {
                "type": "ARRAY",
                "description": "List of drugs potentially responsible for DILI.",
                "items": { "type": "STRING" }
                }
            },
            "required": [
                "chief_complaint",
                "hopi_significant_points",
                "chronic_medications",
                "acute_medication_onset",
                "possibilities_for_dili"
            ]
            },
            "rule_out_complete": {
            "type": "OBJECT",
            "description": "Workup to exclude alternative diagnoses or etiologies.",
            "properties": {
                "viral_hepatitis": {
                "type": "STRING",
                "description": "Status of viral hepatitis workup including HAV, HBV, and HCV serologies."
                },
                "autoimmune": {
                "type": "STRING",
                "description": "Status of autoimmune liver disease assessment including ANA, SMA, or other markers."
                },
                "other_competing_dx_ruled_out": {
                "type": "STRING",
                "description": "Narrative describing exclusion of other liver disease causes."
                }
            },
            "required": [
                "viral_hepatitis",
                "autoimmune",
                "other_competing_dx_ruled_out"
            ]
            },
            "injury_pattern_trends": {
            "type": "OBJECT",
            "description": "Pattern of liver injury and relevant diagnostic calculations.",
            "properties": {
                "pattern": {
                "type": "STRING",
                "description": "Primary injury pattern classification (e.g., hepatocellular, cholestatic, mixed)."
                },
                "hys_law": {
                "type": "STRING",
                "description": "Hy's Law assessment describing severity and prognostic risk."
                },
                "meld_na": {
                "type": "STRING",
                "description": "Clinical interpretation of MELD-Na severity."
                },
                "lft_data_peak_onset": {
                "type": "OBJECT",
                "description": "Peak abnormal liver test values at time of injury recognition.",
                "properties": {
                    "ALT": { "type": "STRING", "description": "Peak ALT level with unit." },
                    "AST": { "type": "STRING", "description": "Peak AST level with unit." },
                    "Alk_Phos": { "type": "STRING", "description": "Peak alkaline phosphatase level with unit." },
                    "T_Bili": { "type": "STRING", "description": "Peak total bilirubin with unit." },
                    "INR": { "type": "STRING", "description": "INR value at peak or presentation." }
                },
                "required": ["ALT", "AST", "Alk_Phos", "T_Bili", "INR"]
                },
                "lft_sparklines_trends": {
                "type": "STRING",
                "description": "Summary of temporal trend in liver enzymes."
                },
                "complications": {
                "type": "ARRAY",
                "description": "List of complications identified during presentation or course.",
                "items": { "type": "STRING" }
                },
                "noh_graz_law": {
                "type": "STRING",
                "description": "Applicability of N-acetyl-p-benzoquinone imine (NAPQI) toxicity rules (for paracetamol/APAP)."
                }
            },
            "required": [
                "pattern",
                "hys_law",
                "meld_na",
                "lft_data_peak_onset",
                "lft_sparklines_trends",
                "complications",
                "noh_graz_law"
            ]
            },
            "severity_prognosis": {
            "type": "OBJECT",
            "description": "Assessment of severity and short-term clinical prognosis.",
            "properties": {
                "severity_features": {
                "type": "ARRAY",
                "description": "Important severity indicators such as encephalopathy, coagulopathy, or high bilirubin.",
                "items": { "type": "STRING" }
                },
                "prognosis_statement": {
                "type": "STRING",
                "description": "Narrative summary of expected clinical course and risk."
                }
            },
            "required": ["severity_features", "prognosis_statement"]
            },
            "key_diagnostics": {
            "type": "OBJECT",
            "description": "Key diagnostic tests performed, ordered, or pending.",
            "properties": {
                "imaging_performed": {
                "type": "STRING",
                "description": "Summary of imaging performed or planned."
                },
                "biopsy": {
                "type": "STRING",
                "description": "Liver biopsy status or findings."
                },
                "methotrexate_level": {
                "type": "STRING",
                "description": "MTX level ordered or obtained during evaluation."
                }
            },
            "required": ["imaging_performed", "biopsy", "methotrexate_level"]
            },
            "management_monitoring": {
            "type": "OBJECT",
            "description": "Management steps, active treatments, consultations and monitoring plans.",
            "properties": {
                "stopped_culprit_drugs": {
                "type": "ARRAY",
                "description": "List of medications stopped due to suspected toxicity.",
                "items": { "type": "STRING" }
                },
                "active_treatments": {
                "type": "ARRAY",
                "description": "List of ongoing treatments and interventions.",
                "items": { "type": "STRING" }
                },
                "consults_initiated": {
                "type": "ARRAY",
                "description": "Specialists consulted for management.",
                "items": { "type": "STRING" }
                },
                "nutrition": {
                "type": "STRING",
                "description": "Nutritional management decisions."
                },
                "vte_ppx": {
                "type": "STRING",
                "description": "Notes regarding venous thromboembolism prophylaxis."
                },
                "causality_rucam": {
                "type": "STRING",
                "description": "RUCAM score and interpretation regarding drug causality."
                },
                "monitoring_plan": {
                "type": "ARRAY",
                "description": "List of monitoring steps or schedules.",
                "items": { "type": "STRING" }
                }
            },
            "required": [
                "stopped_culprit_drugs",
                "active_treatments",
                "consults_initiated",
                "nutrition",
                "vte_ppx",
                "causality_rucam",
                "monitoring_plan"
            ]
            },
            "current_status_last_48h": {
            "type": "STRING",
            "description": "Clinical status over the last 48 hours, including improvement, deterioration, or stability."
            }
        },
        "required": [
            "name",
            "mrn",
            "age_sex",
            "date_of_summary",
            "one_sentence_impression",
            "clinical_context_baseline",
            "suspect_drug_timeline",
            "rule_out_complete",
            "injury_pattern_trends",
            "severity_prognosis",
            "key_diagnostics",
            "management_monitoring",
            "current_status_last_48h"
        ]
        }



    model = genai.GenerativeModel(
        MODEL,
        system_instruction=SYSTEM_PROMPT_PATIENT,
    )
    print(f"Running generate_patient_report model")
    ehr_data = await load_ehr()
    prompt = f"""Please generate Patient Report object.


        This is patient raw data : {ehr_data}"""

    resp = model.generate_content(
        prompt,
        generation_config={
            "response_mime_type": "application/json",
            "response_schema": RESPONSE_SCHEMA
        }
        )
    result = json.loads(resp.text)
    object_data = {
        "patientData" : result
    }

    with open(f"{config.output_dir}/patient_report_object.json", "w", encoding="utf-8") as f:
        json.dump(object_data, f, ensure_ascii=False, indent=4)
    return object_data

def create_diagnosis(payload):
    print("Start create object")
    url = BASE_URL + "/api/diagnostic-report"
    payload['zone'] = "dili-analysis-zone"
    with open(f"{config.output_dir}/diagnosis_create_payload.json", "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=4)
    response = requests.post(url, json=payload)
    print(response.status_code, "Create diagnosis")
    with open(f"{config.output_dir}/diagnosis_create_response.json", "w", encoding="utf-8") as f:
        json.dump(response.json(), f, ensure_ascii=False, indent=4)   

async def create_dili_diagnosis():
    print("Start generate DILI object")
    diagnosis_content = await generate_dili_diagnosis()

    print("Diagnosis content generated")

    # canvas_ops.create_diagnosis(diagnosis_content)
    create_diagnosis(diagnosis_content)

def create_report(payload):
    print("Start create object")
    url = BASE_URL + "/api/patient-report"
    payload['zone'] = "patient-report-zone"
    response = requests.post(url, json=payload)
    print(response.status_code)

async def create_patient_report():
    diagnosis_content = await generate_patient_report()

    print("Patient report content generated")

    create_report(diagnosis_content)



def create_legal(payload):
    print("Start legal object")
    url = BASE_URL + "/api/legal-compliance"

    with open(f"{config.output_dir}/legal_create_payload.json", "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=4)
    response = requests.post(url, json=payload)
    print(response.status_code)
    data = response.json()

    with open(f"{config.output_dir}/legal_create_response.json", "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)


async def create_legal_doc():
    with open("output/legal_object.json", "r", encoding="utf-8") as f:
        legal_payload = json.load(f)

    create_legal(legal_payload)
# start_time = time.time()

# # query = "Pull radiology data for Sarah Miller"
# # parse_tool(query)
# asyncio.run(generate_patient_report())
# asyncio.run(generate_dili_diagnosis())
# asyncio.run(generate_easl_diagnosis())
# end_time = time.time()
# execution_time = end_time - start_time
# print(f"Execution time: {execution_time:.4f} seconds")